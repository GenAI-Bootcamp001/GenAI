{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHbnulegscow"
      },
      "source": [
        "# Building a Basic RAG App with Gradio UI - A Guide\n",
        "This guide outlines the components of a basic Retrieval-Augmented Generation (RAG) application built using the Gradio framework.\n",
        "\n",
        "# Understanding the Application Flow:\n",
        "The application interacts with the user through functions triggered by specific actions. Let's break down the key functions and their roles:\n",
        "\n",
        "# process_pdf:\n",
        "This function is the engine that kicks in when you upload a PDF file and click the \"Analyse\" button. It takes over the uploaded document and performs the following tasks:\n",
        "\n",
        "    Extraction: Extracts the text content from the PDF file.\n",
        "    Splitting: Divides the extracted text into manageable units (e.g., sentences or paragraphs) for further processing.\n",
        "    Storage: Stores the processed text data for later use.\n",
        "    Chain creation: Generates \"chains\" based on the processed text. These chains are essential for the interaction between the user and the system (explained further in the PDF_Processor section).\n",
        "\n",
        "\n",
        "\n",
        "# PDF_Processor Class:\n",
        "This class acts as a dedicated worker for handling the PDF input. It encapsulates the functionalities mentioned in the process_pdf function: extracting, splitting, and storing the PDF content. Additionally, it plays a crucial role in creating the aforementioned \"chains\" - a concept unique to RAG applications. These chains are likely data structures that link relevant information within the processed document, crucial for the system to retrieve contextual answers during user interaction.\n",
        "\n",
        "# QandA_response_handler:\n",
        "This function tackles user queries in a question-answering format. It performs the following steps:\n",
        "\n",
        "    Interaction: Takes the user's question as input.\n",
        "    Response Generation: Triggers the llm_chat_query function to send the refined query to the large language model (LLM) and retrieves the answer generated by the LLM.\n",
        "\n",
        "#chatbot_response_handler:\n",
        "This function is responsible for handling user input in a broader conversational format. It likely operates as follows:\n",
        "\n",
        "    Input Acquisition: Receives the user's message as input.\n",
        "    LLM Interaction: Triggers the llm_chat_query function to send the user's message to the LLM.\n",
        "    Response Retrieval: Receives the response generated by the LLM based on the user's message.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nuiDhbqL5hp1",
        "outputId": "d9c0a343-d52c-42a0-8f4f-5135cd8c632a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain_community in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (0.2.16)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain_community) (2.0.34)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain_community) (3.10.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain_community) (0.2.16)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain_community) (0.2.38)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain_community) (0.1.117)\n",
            "Requirement already satisfied: numpy<2,>=1 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: requests<3,>=2 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain_community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.11.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (0.2.4)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (2.9.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langchain-core<0.3.0,>=0.2.38->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
            "Requirement already satisfied: anyio in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (4.4.0)\n",
            "Requirement already satisfied: httpcore==1.* in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.0->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (2.23.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This code installs several Python packages that are required for the chatbot application:\n",
        "\n",
        "- `gradio`: A Python library for creating interactive web applications.\n",
        "- `openai`: The official Python client library for the OpenAI API, which is used for language modeling and generation.\n",
        "- `PyMUPDF`: A Python wrapper for the MuPDF library, which is used for reading and manipulating PDF files.\n",
        "- `chromadb`: A Python library for storing and querying vector embeddings, which is used for the chatbot's knowledge base.\n",
        "- `langchain_openai`: Provides integration between LangChain and OpenAI models.\n",
        "- `langchain`: A framework for building applications with large language models.\n",
        "- `langchain_community`: Additional community-contributed functionality for LangChain.\n",
        "These packages are installed using the `pip install` command, which is a package installer for Python. The `-q` flag is used to suppress the output of the installation process.\n",
        "\"\"\"\n",
        "\n",
        "!pip install gradio -q\n",
        "!pip install openai -q\n",
        "!pip install PyMUPDF -q\n",
        "!pip install chromadb -q\n",
        "!pip install langchain_openai -q\n",
        "!pip install langchain -q\n",
        "!pip install langchain_community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_0wt7TbL4dX9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kranthivardhankurumindla/Desktop/GENAI_BOOTCAMP/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "This code imports various libraries and modules that are commonly used in building chatbots and natural language processing applications. The libraries include:\n",
        "\n",
        "- `gradio`: A library for creating interactive web applications.\n",
        "- `os`: A module for interacting with the operating system.\n",
        "- `openai`: A library for accessing the OpenAI API.\n",
        "- `fitz`: A library for working with PDF documents.\n",
        "- `pandas`: A library for data manipulation and analysis.\n",
        "- `langchain`: A library for building language models and chains.\n",
        "- `chromadb`: A library for storing and querying vector embeddings.\n",
        "- `datetime`: A module for working with dates and times.\n",
        "\n",
        "These libraries are likely used in the larger context of the chatbot application to handle tasks such as text processing, document analysis, and model integration.\n",
        "\"\"\"\n",
        "import gradio as gr\n",
        "import os\n",
        "import openai\n",
        "import fitz  #imported from PyMUPDF\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "import time\n",
        "import chromadb #vectorstore\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gBSlSZpr4dzy"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Sets the OpenAI API key as an environment variable.\n",
        "\n",
        "This code sets the OPENAI_API_KEY environment variable to the provided API key. This allows the OpenAI API to be used throughout the application without needing to explicitly pass the API key.\n",
        "\"\"\"\n",
        "import getpass\n",
        "# Set OpenAI API key\n",
        "OPENAI_API_KEY = getpass.getpass()\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "zEH1ddHTZaI-"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Initialize the ChatOpenAI language model with the specified parameters.\n",
        "\n",
        "Args:\n",
        "    model (str): The name of the GPT-3.5 model to use, e.g. 'gpt-3.5-turbo-0125'.\n",
        "    temperature (float): The temperature parameter to control the randomness of the model's output, typically between 0 and 1.\n",
        "\n",
        "Returns:\n",
        "    ChatOpenAI: An instance of the ChatOpenAI language model.\n",
        "\"\"\"\n",
        "llm = ChatOpenAI(model='gpt-3.5-turbo-0125', temperature=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "9DSPJewP5HJp"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.prompts import (\n",
        "    ChatPromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "Loads PDF and Word documents, extracts text, splits text into chunks,\n",
        "vectorizes chunks for semantic search, and sets up a QA retrieval\n",
        "pipeline.\n",
        "\n",
        "load_and_split extracts text from PDF and Word documents.\n",
        "\n",
        "vectorstore_and_chain vectorizes text chunks and sets up a QA\n",
        "retrieval pipeline using Chroma/Anthropic.\n",
        "\n",
        "extract_document_keywords extracts keywords/entities from extracted\n",
        "text.\n",
        "\"\"\"\n",
        "\n",
        "class PDFprocessor:\n",
        "\n",
        "\n",
        "    def load_and_split(self,filepaths):\n",
        "      \"\"\"\n",
        "      Loads and splits PDF documents into a list of Document objects.\n",
        "\n",
        "      Args:\n",
        "          filepaths (list): A list of file paths to PDF documents.\n",
        "\n",
        "      Returns:\n",
        "          list: A list of Document objects, where each Document contains a text chunk and its associated metadata.\n",
        "      \"\"\"\n",
        "      documents=[]\n",
        "      for file_path in filepaths:\n",
        "        if file_path.endswith(\".pdf\"):\n",
        "           loader = PyMuPDFLoader(file_path)\n",
        "           documents.extend(loader.load())\n",
        "      text_splitter = RecursiveCharacterTextSplitter(\n",
        "            # Set a really small chunk size, just to show.\n",
        "            chunk_size=100,\n",
        "            chunk_overlap=20,\n",
        "            length_function=len,\n",
        "            is_separator_regex=False,\n",
        "        )\n",
        "      docs=text_splitter.split_documents(documents)\n",
        "\n",
        "      return docs\n",
        "\n",
        "\n",
        "      \"\"\"\n",
        "      Generates a Chroma vector database from a list of documents, creates a RetrievalQA chain using the vector database, and returns the chain.\n",
        "\n",
        "      Args:\n",
        "          documents (list): A list of Document objects, where each Document contains a text chunk and its associated metadata.\n",
        "\n",
        "      Returns:\n",
        "          RetrievalQA: A RetrievalQA chain that can be used to answer questions based on the provided documents.\n",
        "      \"\"\"\n",
        "    def vectorstore_and_chain(self, docs):\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Creates an OpenAIEmbeddings instance using the \"text-embedding-ada-002\" model.\n",
        "\n",
        "        The OpenAIEmbeddings class is used to generate vector embeddings for text using the OpenAI language model. The \"text-embedding-ada-002\" model is a general-purpose text embedding model that can be used for a variety of natural language processing tasks.\n",
        "        \"\"\"\n",
        "        embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
        "        current_time = datetime.now()\n",
        "        formatted_time = current_time.strftime(\"%Y-%m-%d%H%M%S\")\n",
        "        print(\"Current Time:\", formatted_time)\n",
        "        collection_name=formatted_time #need to give a unique name to the collection of vectors this is done to avoid the retrieval of previous instance pdfs\n",
        "        if not should_stop_analysis:\n",
        "         try:\n",
        "            vectordb = Chroma.from_documents(docs,embeddings,collection_name=collection_name)\n",
        "         except openai.RateLimitError as e:\n",
        "            print(\"Rate limit exceeded, waiting before retrying...\")\n",
        "            # time.sleep(60)\n",
        "            vectordb = Chroma.from_documents(docs, embeddings,collection_name=collection_name)\n",
        "        else:\n",
        "            vectordb=Chroma.from_documents([],embeddings,collection_name)\n",
        "        template = \"\"\"Imagine you are a good question answering system and you answer questions based on the Relevant information.\n",
        "        Don't make up any information\\n\n",
        "        Also, include all the relevant numerical figures\\n\n",
        "        Recheck your answer so that it is more coherent with what user is asking\\n\n",
        "        Relevant information:{context}\n",
        "        Question: For the company,{question}\n",
        "        Answer:\"\"\"\n",
        "        global chain\n",
        "        QA_CHAIN_PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=template)\n",
        "        retriever=vectordb.as_retriever(search_kwargs={\"k\": 5})\n",
        "        \"\"\"\n",
        "        Initializes a RetrievalQA chain using the provided language model (llm) and retriever.\n",
        "\n",
        "        The chain is configured to return the source documents along with the answer, and the prompt is set using the QA_CHAIN_PROMPT parameter.\n",
        "\n",
        "        Args:\n",
        "            llm (LLMChain): The language model to use for the chain.\n",
        "            retriever (Retriever): The retriever to use for the chain.\n",
        "            QA_CHAIN_PROMPT (str): The prompt to use for the QA chain.\n",
        "\n",
        "        Returns:\n",
        "            RetrievalQA: The initialized RetrievalQA chain.\n",
        "        \"\"\"\n",
        "        chain = RetrievalQA.from_chain_type(\n",
        "            llm,\n",
        "            retriever=retriever,\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        "        )\n",
        "        memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
        "        system_message = SystemMessagePromptTemplate.from_template(\n",
        "        \"\"\"Imagine you are a good question answering system and you answer questions based on the Relevant information.\n",
        "        Don't make up any information\\n\n",
        "        Also, include all the relevant numerical figures\\n\n",
        "        Recheck your answer so that it is more coherent with what user is asking\\n\n",
        "        Relevant information:{context}\n",
        "        Question: For the company,{question}\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "    )\n",
        "        human_message = human_message_prompt = HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "        conv_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vectordb.as_retriever(search_kwargs={\"k\": 5}),\n",
        "        memory=memory,\n",
        "        combine_docs_chain_kwargs={\n",
        "            \"prompt\": ChatPromptTemplate.from_messages(\n",
        "                [\n",
        "                    system_message,\n",
        "                    human_message,\n",
        "                ]\n",
        "            ),\n",
        "        },\n",
        "    )\n",
        "\n",
        "        return chain,conv_chain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "gQSrK8kmaYN9"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Processes a list of PDF file paths and returns the processed data or an error message.\n",
        "\n",
        "Args:\n",
        "    filepaths (list): A list of file paths to PDF files.\n",
        "\n",
        "Returns:\n",
        "    str: A message indicating the result of the PDF processing, or an error message.\n",
        "    Any: The processed data, or None if an error occurred.\n",
        "\"\"\"\n",
        "\n",
        "def process_pdf(filepaths):\n",
        "    global should_stop_analysis\n",
        "    should_stop_analysis = False\n",
        "    global processed_data_cache\n",
        "\n",
        "    if not filepaths or not isinstance(filepaths, list) or not filepaths[0]:\n",
        "        return \"Please upload a valid PDF file.\", None\n",
        "    if should_stop_analysis:\n",
        "        print(should_stop_analysis)\n",
        "        return \"Analysis stopped by user.\", None\n",
        "    else:\n",
        "      pdf_processing=PDFprocessor()\n",
        "      documents=pdf_processing.load_and_split(filepaths)\n",
        "      global chain,conv_chain\n",
        "      chain,conv_chain=pdf_processing.vectorstore_and_chain(documents)\n",
        "      return \"Processed pdf\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Yac0MZXga3qK"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Handles incoming chat messages by preparing document context, querying the LLM,\n",
        "and returning the response.\n",
        "\n",
        "Parameters:\n",
        "  message (str): The incoming chat message.\n",
        "  history (list): The chat history so far.\n",
        "\n",
        "Returns:\n",
        "  str: The generated response for the given message and context.\n",
        "\"\"\"\n",
        "async def QandA_response_handler(message, history):\n",
        "\n",
        "    print(\"Processing chat response...\")\n",
        "    response = await dynamic_llm_query(message,history)\n",
        "    print(f\"Message: {message}, Response: {response}\")\n",
        "\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AyRJCwLia7V6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Handles querying the LLM to generate a response for an incoming chat message.\n",
        "\n",
        "Uses a RetrievalQA chain if available to query the document context.\n",
        "Otherwise falls back to querying the LLM directly.\n",
        "\n",
        "Parameters:\n",
        "  message (str): The incoming chat message.\n",
        "\n",
        "Returns:\n",
        "  str: The generated response.\n",
        "\"\"\"\n",
        "\n",
        "async def dynamic_llm_query(message,history):\n",
        "    # Retrieve the QA chain, if available\n",
        "    global chain\n",
        "    if chain:\n",
        "        try:\n",
        "            # Use the QA chain for detailed document-related queries\n",
        "            response = chain.invoke({\"query\": message,\"chat_history\":history}, max_tokens=300)\n",
        "            answer = response[\"result\"]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error querying RetrievalQA chain: {e}\")\n",
        "            answer = \"An error occurred while querying the RetrievalQA chain.\"\n",
        "    else:\n",
        "        # Fallback or additional logic for using the LLM directly\n",
        "        answer = \"If document is uploaded please wait until the document processes or if it is not uploaded please upload a file\"\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "cZKKdzjTUwDh"
      },
      "outputs": [],
      "source": [
        "async def chatbot_response_handler(message, history):\n",
        "\n",
        "    print(\"Processing chat response...\")\n",
        "    response = await dynamic_llm_chat_query(message,history)\n",
        "    print(f\"Message: {message}, Response: {response}\")\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "m0YsJRw0VGPD"
      },
      "outputs": [],
      "source": [
        "\n",
        "async def dynamic_llm_chat_query(message,history):\n",
        "# Retrieve the QA chain, if available\n",
        "    global conv_chain\n",
        "    if conv_chain:\n",
        "        try:\n",
        "            # Use the QA chain for detailed document-related queries\n",
        "            response = conv_chain.invoke({\"question\": message,\"chat_history\":history[-5:]}, max_tokens=300)\n",
        "            answer = response[\"answer\"]\n",
        "        except Exception as e:\n",
        "            print(f\"Error querying RetrievalQA chain: {e}\")\n",
        "            answer = \"An error occurred while querying the RetrievalQA chain.\"\n",
        "    else:\n",
        "        # Fallback or additional logic for using the LLM directly\n",
        "        answer = \"If document is uploaded please wait until the document processes or if it is not uploaded please upload a file\"\n",
        "\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "tl6PqPKGa-Z6"
      },
      "outputs": [],
      "source": [
        "general_questions=[\n",
        "    \"Under what circumstances can the policy be canceled?\",\n",
        "    \"Are there any provisions for renewal or cancellation?\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q8-00z0ebOIU",
        "outputId": "d98dccf4-0dce-4781-cdb0-5030377d04e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Time: 2024-09-23172049\n",
            "Processing chat response...\n",
            "Message: What is the employee agreement about, Response: The employee agreement outlines the salary, rights, and any other benefits of the Executive under this Agreement or as an employee. It also specifies that during the Employment Term, the Agreement may terminate without further compensation obligations.\n",
            "Processing chat response...\n",
            "Message: what is the compensation , Response: The compensation for the company includes a combination of base salary, bonuses, and long-term incentives. The long-term incentives are payable for the achievement of performance goals established by the Compensation Committee. The amount of long-term incentives can be up to a certain amount determined by the committee. Additionally, other benefits and compensation under existing plans are also provided based on the terms and rules of those plans.\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\"\"\"\n",
        "Analyzes PDF documents uploaded by the user.\n",
        "\n",
        "Outputs analysis results and a CSV download.\n",
        "\"\"\"\n",
        "def main():\n",
        "    custom_css = \"\"\"\n",
        "    .header-text h1, .header-text h2 {\n",
        "        text-align: center;    }\n",
        "    .instruction-text {\n",
        "        text-align: justify;\n",
        "        margin: 20px;\n",
        "        font-size: 18px;\n",
        "    }\n",
        "    .output-container {\n",
        "        max-height: 500px; /* Adjust based on your preference */\n",
        "        overflow-y: auto;\n",
        "    }\n",
        "    \"\"\"\n",
        "    theme = gr.themes.Soft(\n",
        "    primary_hue=\"rose\",\n",
        "    secondary_hue=\"rose\",\n",
        "    font=[gr.themes.GoogleFont('Poppins'), 'ui-sans-serif', 'system-ui', 'sans-serif'],\n",
        ").set()\n",
        "    # def clear_all():\n",
        "    #             file_upload.value = []  # Clear selected files\n",
        "    #             output_container.value = \"\"\n",
        "    with gr.Blocks(css=custom_css, theme=theme) as demo:\n",
        "        # with gr.Row():\n",
        "        #    with gr.Column():\n",
        "        #          Header texts\n",
        "        #         gr.HTML(\"<div class='header-text'><h1>Risk Analyser Tool</h1></div>\")\n",
        "        #         gr.HTML(\"<div class='header-text'><h2>Analysing Reports Made Easy</h2></div>\")\n",
        "        #         gr.HTML(\"<div class='header-text'><h3>Get insights from your PDF document instantly</h3></div>\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                gr.Markdown(\"## Upload and Analyze PDF\")\n",
        "                with gr.Group():\n",
        "                    file_upload = gr.Files(show_label=False,file_count=\"multiple\", file_types=[\".pdf\",\"pdf\"])\n",
        "                    analyze_button = gr.Button(\"Analyze PDF\")\n",
        "                    analyze_button.click(\n",
        "                    fn=process_pdf,\n",
        "                    inputs=file_upload,\n",
        "                    outputs=gr.Text(show_label=False)\n",
        "                )\n",
        "\n",
        "                clear=gr.ClearButton(components=[file_upload],value=\"Clear\")\n",
        "\n",
        "                \"\"\"\n",
        "                Stops the ongoing analysis and resets the analysis chain.\n",
        "\n",
        "                This function is called when the \"Clear\" button is clicked. It sets a global flag `should_stop_analysis` to `True` to signal that the analysis should be stopped. It also sets the `chain` global variable to `None` to reset the analysis chain.\n",
        "\n",
        "                A warning message is displayed to the user, advising them to wait for approximately 20 seconds if they have interrupted the processing.\n",
        "                \"\"\"\n",
        "                def on_file_clear():\n",
        "                    global should_stop_analysis\n",
        "                     # Set the flag to stop the analysis\n",
        "                    should_stop_analysis = True\n",
        "                    global chain\n",
        "                    chain =None\n",
        "                    gr.Warning(\"If you have interrupted the processing please wait for 20s\")\n",
        "\n",
        "                    #demo.close()\n",
        "                clear.click(\n",
        "                    fn=on_file_clear\n",
        "                )\n",
        "        with gr.Tab(\"Question Answering\"):\n",
        "          with gr.Column():\n",
        "            gr.ChatInterface(\n",
        "                fn=QandA_response_handler,\n",
        "                examples=general_questions,\n",
        "                retry_btn=None,\n",
        "                undo_btn=None,\n",
        "                clear_btn=\"Clear\",\n",
        "              )\n",
        "        with gr.Tab(\"Chatbot\"):\n",
        "          with gr.Column():\n",
        "            gr.ChatInterface(\n",
        "                fn=chatbot_response_handler,\n",
        "                examples=general_questions,\n",
        "                retry_btn=None,\n",
        "                undo_btn=None,\n",
        "                clear_btn=\"Clear\",\n",
        "              )\n",
        "        with gr.Row():\n",
        "             # Instructions\n",
        "             gr.HTML(\"<div class='instruction-text'>\"\n",
        "                     \"<h2><strong>Instructions:</strong></h2><br>\"\n",
        "                     \"1. Upload the file and click on the Analyse button.<br>\"\n",
        "                     \"2. Please hold on until the processing is complete and you see a text confirmation processed pdf<br>\"\n",
        "                     \"4.Click on the 'Clear' button to clear the file and the output.<br>\"\n",
        "                     \"Note: If you want to stop the processing in the middle,click on the Clear Button and wait for approx 20s to clear the file and the output.<br>\"\n",
        "                     \"</div>\")\n",
        "\n",
        "\n",
        "    demo.launch(debug=True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
