{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"f9yT9jl2seQL"},"outputs":[],"source":["!pip install openai"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ylZ3rxjBHcIK"},"outputs":[],"source":["!pip install llama-index\n","!pip install pypdf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GBmPKEp7M9_k"},"outputs":[],"source":["%pip install llama-index-readers-google"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36867,"status":"ok","timestamp":1715051524664,"user":{"displayName":"Jomon Jesudas","userId":"04523114763155887947"},"user_tz":-330},"id":"Z8NS9ca2oK3g","outputId":"87f49d05-17a7-41ae-e806-20ee371ad481"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RYMxXTSegLF-"},"outputs":[],"source":["## Retrieval augmented generation\n","\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T6sJu5kiiM9R"},"outputs":[],"source":["api_key = r'apikey'"]},{"cell_type":"markdown","metadata":{"id":"FK7giYZ4pDGu"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9o0ro1y2pK1A"},"outputs":[],"source":["os.environ['OPENAI_API_KEY'] = api_key"]},{"cell_type":"markdown","metadata":{"id":"Zjm64O8mt3WB"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1605,"status":"ok","timestamp":1715060722027,"user":{"displayName":"Jomon Jesudas","userId":"04523114763155887947"},"user_tz":-330},"id":"BfIegqYbpoHb","outputId":"a32653fe-0547-4f6d-e9c6-7c193fd21a17"},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","  \"winner\": \"Los Angeles Dodgers\",\n","  \"year\": 2020\n","}\n"]}],"source":["## test open ai api key\n","\n","from openai import OpenAI\n","client = OpenAI()\n","\n","response = client.chat.completions.create(\n","  model=\"gpt-3.5-turbo-0125\",\n","  response_format={ \"type\": \"json_object\" },\n","  messages=[\n","    {\"role\": \"system\", \"content\": \"You are a helpful assistant designed to output JSON.\"},\n","    {\"role\": \"user\", \"content\": \"Who won the world series in 2020?\"}\n","  ]\n",")\n","print(response.choices[0].message.content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6z_jtYcHqVAe"},"outputs":[],"source":["from llama_index.core import VectorStoreIndex,SimpleDirectoryReader\n","from llama_index.readers.google import GoogleDriveReader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PnfRsdzTLW3y"},"outputs":[],"source":["reader = SimpleDirectoryReader(input_dir=\"/content/data/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BF7p15spIYzn"},"outputs":[],"source":["documents = reader.load_data()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":433,"status":"ok","timestamp":1715061590371,"user":{"displayName":"Jomon Jesudas","userId":"04523114763155887947"},"user_tz":-330},"id":"gir8GCoNLn9G","outputId":"f920499e-b33b-448b-a477-8131f6eac1c5"},"outputs":[{"data":{"text/plain":["[Document(id_='d78c3e7f-169f-4a6a-9b2e-4f09b9c3ba56', embedding=None, metadata={'page_label': '1', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Elvis SaraviaPrompt EngineeringA lecture by DAIR.AI', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='19c9e9e6-47c6-44fe-9a8a-9e1dae1bb80f', embedding=None, metadata={'page_label': '2', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prerequisites & Objectives•Prerequisites: •Python •Knowledge of language models •Basic understanding of deep learning / ML concepts •Objectives •Present an introduction to prompt engineering •Present an overview of the latest prompting techniques  •Provide demonstrations and exercises to practice different prompting techniques', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='da8501ff-e0cb-40b9-9162-01a17ad3b8e3', embedding=None, metadata={'page_label': '3', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Agenda•Introduction to Prompt Engineering •Advanced Techniques for Prompt Engineering •Applications & Tools •Conclusion & Future Directions', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='d079efdb-90d2-43fb-a2c1-565bf4c56de6', embedding=None, metadata={'page_label': '4', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Introduction to Prompt EngineeringPart 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='8ce87cc0-1e50-4b7c-9059-2b218f1da283', embedding=None, metadata={'page_label': '5', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='What are prompts?•Prompts involve instructions and context passed to a language model to achieve a desired task •Prompt engineering is the practice of developing and optimizing prompts to efficiently use language models (LMs) for a variety of applications  •Prompt engineering is a useful skill for AI engineers and researchers to improve and efficiently use language modelsWhat is prompt engineering?Prompt engineering is a process of creating a set of prompts, or questions, that are used to guide the user toward a desired outcome. It is an effective tool for designers to create user experiences that are easy to use and intuitive. This method is often used in interactive design and software development, as it allows users to easily understand how to interact with a system or product..', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='eaf6ef9b-0fab-4251-8b04-35c4e5e88660', embedding=None, metadata={'page_label': '6', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Why Prompt Engineering?•Why learn prompt engineering? •Important for research, discoveries, and advancement •Helps to test and evaluate the limitations of LLMs •Enables all kinds of innovative applications on top of LLMs \\nSource: https://jobs.lever.co/Anthropic/e3cde481-d446-460f-b576-93cab67bd1ed', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='b5b76530-425f-4dd1-a7eb-5ff04f092386', embedding=None, metadata={'page_label': '7', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='First Basic PromptThe sky is blue The sky is a beautiful blue color during the day. The blue of the sky is created by the Earth’s atmosphere scattering the sun’s light. The blue is a result of the air molecules in the atmosphere reflecting the shorter wavelength of blue light back to our eyes.Model: “text-davinci-003” temperature: 0.7 top-p: 1InstructionResponse', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='80c51e1b-cf83-4416-9415-78651399d788', embedding=None, metadata={'page_label': '8', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Elements of a Prompt•A prompt is composed with the following components: •Instructions •Context •Input data •Output indicatorClassify the text into neutral, negative or positiveText: I think the food was okay.Sentiment: ', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='8bd3ab0b-0e64-4801-ab38-984ec947e2f0', embedding=None, metadata={'page_label': '9', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Settings to keep in mind•When prompting a language model you should keep in mind a few settings •You can get very different results with prompts when using different settings •One important setting is controlling how deterministic the model is when generating completion for prompts •Temperature and top_p are two important parameters to keep in mind •Generally, keep these low if you are looking for exact answers •…keep them high if you are looking for more diverse responses', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='e58a6e8b-ade9-4ab4-9500-a0eeec44d9e4', embedding=None, metadata={'page_label': '10', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Designing Prompts for Different Tasks•In the next few slides, we will cover a few examples of common tasks using different prompts •Tasks covered: •Text Summarization •Question Answering •Text Classification •Role Playing •Code Generation •Reasoning', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='ca01999e-f97d-46bb-b26b-3299a8095c40', embedding=None, metadata={'page_label': '11', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Text SummarizationAntibiotics are a type of medication used to treat bacterial infections. They work by either killing the bacteria or preventing them from reproducing, allowing the body’s immune system to fight off the infection. Antibiotics are usually taken orally in the form of pills, capsules, or liquid solutions, or sometimes administered intravenously. They are not effective against viral infections, and using them inappropriately can lead to antibiotic resistance.Explain the above in one sentence: Antibiotics are medications used to treat bacterial infections by either killing the bacteria or stopping them from reproducing, but they are not effective against viruses and overuse can lead to antibiotic resistance.ContextInstruction', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='3123ed52-b8e0-4438-984e-1a18ab9d7512', embedding=None, metadata={'page_label': '12', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Question AnsweringAnswer the question based on the context below. Keep the answer short and concise. Respond \"Unsure about answer\" if not sure about the answer.Context: Teplizumab traces its roots to a New Jersey drug company called Ortho Pharmaceutical. There, scientists generated an early version of the antibody, dubbed OKT3. Originally sourced from mice, the molecule was able to bind to the surface of T cells and limit their cell-killing potential. In 1986, it was approved to help prevent organ rejection after kidney transplants, making it the first therapeutic antibody allowed for human use.Question: What was OKT3 originally sourced from?Answer: Mice.\\nContext obtained from: https://www.nature.com/articles/d41586-023-00400-x', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='5763d49c-f310-4d46-a670-418162fc97a1', embedding=None, metadata={'page_label': '13', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Text Classification\\nClassify the text into neutral, negative or positive.Text: I think the food was okay.Sentiment: Neutral', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='b8b16e54-ad07-4469-ab7b-a77646a9fec7', embedding=None, metadata={'page_label': '14', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Role PlayingThe following is a conversation with an AI research assistant. The assistant tone is technical and scientific.Human: Hello, who are you?AI: Greeting! I am an AI research assistant. How can I help you today?Human: Can you tell me about the creation of blackholes?AI: Sure! Black holes are regions of spacetime where the gravitational force is so strong that nothing, not even light, can escape from it. They are created when a very massive star dies and its core collapses in on itself, forming a singularity of infinite density. The intense gravity of the singularity pulls in all the matter and radiation around it, creating the black hole.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='c6d1d572-b9ec-4247-8e60-ad80163e94cb', embedding=None, metadata={'page_label': '15', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text=\"Code Generation“””Table departments, columns = [DepartmentId, DepartmentName]Table students, columns = [DepartmentId, StudentId, StudentName]Create a MySQL query for all students in the Computer Science Department“””SELECT StudentId, StudentName FROM students WHERE DepartmentId IN (SELECT DepartmentId FROM departments WHERE DepartmentName = 'Computer Science');\", start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='f9c7ae9b-4aca-41e3-bac4-972e254ca9d1', embedding=None, metadata={'page_label': '16', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Reasoning\\nThe odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1.Solve by breaking the problem into steps. First, identify the odd numbers, add them, and indicate whether the result is odd or even.Odd numbers: 15, 5, 13, 7, 1Sum: 41 41 is an odd number.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='6b574716-0855-4a38-b579-9b504fe2137f', embedding=None, metadata={'page_label': '17', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='DemoIntroduction to Prompt Engineering', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='698ad6c9-db17-45d2-85bf-fb444b3dca84', embedding=None, metadata={'page_label': '18', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Advanced Techniques for Prompt EngineeringPart 2', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='433e5b3e-0fca-4db1-b99f-7377361bca7e', embedding=None, metadata={'page_label': '19', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt Engineering Techniques•Many advanced prompting techniques have been designed to improve performance on complex tasks •Few-shot prompts •Chain-of-thought (CoT) prompting •Self-Consistency •Knowledge Generation Prompting •ReAct', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='a753d51d-4cc2-4d10-a1b7-eab4e2d388b7', embedding=None, metadata={'page_label': '20', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Few-shot Prompts•Few-shot prompting allows us to provide exemplars in prompts to steer the model towards better performanceThe odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.A: The answer is False.The odd numbers in this group add up to an even number: 17,  10, 19, 4, 8, 12, 24.A: The answer is True.The odd numbers in this group add up to an even number: 16,  11, 14, 4, 8, 13, 24.A: The answer is True.The odd numbers in this group add up to an even number: 17,  9, 10, 12, 13, 4, 2.A: The answer is False.The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. A: The answer is True.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='568858b0-c8c9-4b23-940a-eaf0e2268603', embedding=None, metadata={'page_label': '21', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Chain-of-Thought (CoT) Prompting•Prompting can be further improved by instructing the model to reason about the task when responding •This is very useful for tasks that requiring reasoning •You can combine it with few-shot prompting to get better results •You can also do zero-shot CoT where exemplars are not availableThe odd numbers in this group add up to an even number: 4, 8, 9, 15, 12, 2, 1.A: Adding all the odd numbers (9, 15, 1) gives 25. The answer is False.The odd numbers in this group add up to an even number: 15, 32, 5, 13, 82, 7, 1. A: Adding all the odd numbers (15, 5, 13, 7, 1) gives 41. The answer is False.Source: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='ecd815cc-e429-4158-8213-86533815cf30', embedding=None, metadata={'page_label': '22', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Zero-Shot CoT•Involves adding \"Let\\'s think step by step\" to the original promptI went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?11 applesI went to the market and bought 10 apples. I gave 2 apples to the neighbor and 2 to the repairman. I then went and bought 5 more apples and ate 1. How many apples did I remain with?Let\\'s think step by step.First, you started with 10 apples.You gave away 2 apples to the neighbor and 2 to the repairman, so you had 6 apples left.Then you bought 5 more apples, so now you had 11 apples.Finally, you ate 1 apple, so you would remain with 10 apples.Source: Large Language Models are Zero-Shot Reasoners', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='fbb757be-346f-47f3-b119-ef6f8d55c30f', embedding=None, metadata={'page_label': '23', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Consistency•Self-Consistency aims to improve on the naive greedy decoding used in chain-of-thought prompting •The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer.  •This helps to boost the performance of CoT prompting on tasks involving arithmetic and commonsense reasoningWhen I was 6 my sister was half my age. NowI’m 70 how old is my sister?35Many examples were generated but the model kept responding 35 as the answerSource: Self-Consistency Improves Chain of Thought Reasoning in Language Models', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='3de0d403-c327-43ef-b2a6-6f1dd12441c1', embedding=None, metadata={'page_label': '24', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Self-Consistency ExampleQ: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?A: We start with 15 trees. Later we have 21 trees. The difference must be the number of trees they planted. So, they must have planted 21 - 15 = 6 trees. The answer is 6.Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?A: There are 3 cars in the parking lot already. 2 more arrive. Now there are 3 + 2 = 5 cars. The answer is 5....Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?A: She bought 5 bagels for $3 each. This means she spent 5Q: When I was 6 my sister was half my age. Now I’m 70 how old is my sister?A:When I was 6 my sister was half my age, so she was 3. Now I am 70, so she is 70 - 3 = 67. The answer is 67.When the narrator was 6, his sister was half his age, which is 3. Now that the narrator is 70, his sister would be 70 - 3 = 67 years old. The answer is 67.When the narrator was 6, his sister was half his age, which is 3. Now that the narrator is 70, his sister would be 70 - 3 = 67 years old. The answer is 67.Output 1Output 2Output 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='bc3098f9-d1e5-425b-8c49-32cfbe4bfd21', embedding=None, metadata={'page_label': '25', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generate Knowledge Prompting•This technique involves using additional knowledge provided as part of the context to improve results on complex tasks such as commonsense reasoning •The knowledge used in the context is generated by a model and used in the prompt to make a prediction •Highest-confidence prediction is used\\nSource: Generated Knowledge Prompting for Commonsense Reasoning', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='410ebd05-83a8-4e0e-92a2-05595ad0ecd3', embedding=None, metadata={'page_label': '26', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generate Knowledge Prompting Example•The first step is to generate knowledge. Below is an example of how to generate the knowledge samplesInput: Greece is larger than mexico.Knowledge: Greece is approximately 131,957 sq km, while Mexico is approximately 1,964,375 sq km, making Mexico 1,389% larger than Greece....Input: A rock is the same size as a pebble.Knowledge: A pebble is a clast of rock with a particle size of 4 to 64 millimetres based on the Udden-Wentworth scale of sedimentology. Pebbles are generally considered larger than granules (2 to 4 millimetres diameter) and smaller than cobbles (64 to 256 millimetres diameter).Input: Part of golf is trying to get a higher point total than others.Knowledge:The objective of golf is to play a set of holes in the least number of strokes. A round of golf typically consists of 18 holes. Each hole is played once in the round on a standard golf course. Each stroke is counted as one point, and the total number of strokes is used to determine the winner of the game.Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into a series of holes on a course using the fewest number of strokes. The goal is to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.Knowledge 2Knowledge 1', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='ee26b2ad-3f59-4d53-b684-1e7bf40ace1e', embedding=None, metadata={'page_label': '27', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Generate Knowledge Prompting Example•The knowledge samples are then used to generate knowledge augmented questions to get answer proposals  •The highest-confidence response is selected as final answerQuestion: Part of golf is trying to get a higher point total than others. Yes or No?Knowledge: The objective of golf is...Explain and Answer: No, the objective of golf is not to get a higher point total than others. Rather, the objective is to play a set of holes in the least number of strokes. The total number of strokes is used to determine the winner of the game, not the total number of points.Question: Part of golf is trying to get a higher point total than others. Yes or No?Knowledge: Golf is a precision club-and-ball sport in which competing players (or golfers) use many types of clubs to hit balls into...Explain and Answer: Yes, part of golf is trying to get a higher point total than others. Each player tries to complete the course with the lowest score, which is calculated by adding up the total number of strokes taken on each hole. The player with the lowest score wins the game.High-confidence prediction\\nLow-confidence prediction', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='1e6a82ab-0b53-420c-aa6c-7bf381507974', embedding=None, metadata={'page_label': '28', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Program-aided Language Model (PAL)•Chain-of-thought prompting is a good example of how to steer models to perform better at complex reasoning tasks •However, sometimes CoT is not enough as it depends only on the generated text from the model •Program-aided language models (PAL) uses an LLM to read problems and generate programs as the intermediate reasoning steps  •It offloads the solution step to a runtime such as Python interpreter', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='6bbb2eab-da14-4c3a-8525-7a8dbed781de', embedding=None, metadata={'page_label': '29', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='PAL\\nSource: PAL: Program-aided Language Models', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='83249eae-2724-4206-a14a-eedb62cc1e37', embedding=None, metadata={'page_label': '30', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ReAct•ReAct is a framework where LLMs are used to generate both reasoning traces and task-specific actions in an interleaved manner •Generating reasoning traces allow the model to induce, track, and update action plans, and even handle exceptions •The action step allows to interface with and gather information from external sources such as knowledge bases or environments. •ReAct allows LLMs to interact with external tools to retrieve additional information that leads to more reliable and factual responses', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='fdbb85ad-d52e-4638-9523-8631b74a2ca1', embedding=None, metadata={'page_label': '31', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='ReAct\\nSource: ReAct: Synergizing Reasoning and Acting in Language Models', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='6a9a7ef4-eb85-4632-a6c1-53a4c7ef34d4', embedding=None, metadata={'page_label': '32', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='DemoAdvanced Techniques for Prompt Engineering', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='26e8a31f-7686-4710-84f8-17c2c25deefa', embedding=None, metadata={'page_label': '33', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tools and ApplicationsPart 3', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='03f05fd5-6670-4e70-96e3-2ffbb4278a7a', embedding=None, metadata={'page_label': '34', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Tools & IDEs•There are many tools, libraries, and platforms with different capabilities and functionalities •Capabilities include: •Developing and experimenting with prompts •Evaluating prompts •Versioning and deploying prompts\\nMore tools here: https://github.com/dair-ai/Prompt-Engineering-Guide#tools--libraries', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='ac6a4020-0029-40ee-a2c6-be973a8c56f3', embedding=None, metadata={'page_label': '35', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Applications•Advanced prompting techniques are now allowing all sorts of advanced applications with LLMs •LLMs and external tools/APIs  •Data-augmented Generation •QA with sources •Summarization using sources', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='7715e15d-3484-4c9b-a79b-171181ea8d96', embedding=None, metadata={'page_label': '36', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='LLMs with External Tools•The generative capabilities of LLMs can be combined with an external tool to solve complex problems. •The components you need: •An agent powered by LLM to determine which actions to take •A tool used by the agent to interact with the world (e.g., search API, Wolfram, Python REPL, database lookup) •The LLM that will power the agent', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='3d6d31f0-8052-4806-807e-5e4094cbd989', embedding=None, metadata={'page_label': '37', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Data-Augmented Generation•For many real-world applications there is a need to augment the generation of a model by incorporating external data •Steps involved: •Fetching relevant data •Augmenting prompts with the retrieved data as context  •External data can include: •Document stores •APIs •Databases •User provided data •…', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='ad7d3c03-cff7-450d-952d-c356b714f0ba', embedding=None, metadata={'page_label': '38', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='DemoTools and Applications', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='e82bf7bb-588c-4037-b846-2a1f2fbd95b0', embedding=None, metadata={'page_label': '39', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Conclusion and Future DirectionsPart 4', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='d6040521-b7eb-4bbf-8fa3-612cca8a31d0', embedding=None, metadata={'page_label': '40', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Opportunities and Future Directions•In this section, we discuss the following: •Model safety •Prompt Injection •RLHF •Future directions', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='50195709-e2e7-47e7-b55e-69725fba7f12', embedding=None, metadata={'page_label': '41', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Model Safety•Prompt engineering can be used not only to improve performance but also the reliability of response from a safety perspective •Prompt engineering can help identify risky behaviours of LLMs which can help to reduce harmful behaviours and risks that may arise from language models •There is also a part of the community performing prompt injection to understand the vulnerability of LLMs', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='9eb66d9d-ada4-4930-bd7d-6521322cbba0', embedding=None, metadata={'page_label': '42', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt Injections•It turns out that building with LLMs, like any other systems, comes with challenges that includes safety considerations. •Prompt injections aim to find vulnerabilities in LLMs •Some common issues include: •Prompt Injection •Prompt Leaking •Jailbreaking', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='b90fd77b-3e66-47d0-beae-a154fb7f7847', embedding=None, metadata={'page_label': '43', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt Injection•Prompt injection is used to hijack an LM’s output by injecting an untrusted command that overrides instruction of a prompt •This could easily happen if you just concatenate your prompt with another user generated prompt\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='313e94fb-1dd0-4b43-8754-4a0d0c89cff2', embedding=None, metadata={'page_label': '44', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt Leaking•Prompt leaking aims to force the model to spit out information about its own prompt. •This can lead to leaking of either sensitive, private or information that’s confidential\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='5f25567a-6536-4681-89b9-22cbc2e43d99', embedding=None, metadata={'page_label': '45', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Jailbreaking•Jailbreaking is another form of prompt injection where the goal is to bypass safety and moderation features •LLMs provided via APIs might be coupled with safety features or content moderation which can be bypassed with harmful prompts/attacks •This might sound like a difficult task but it’s not because the model is usually served static and might have these vulnerabilities due to many factors such as the data it was trained on, etc.', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='98c5bbe7-1c70-47eb-986b-47629a39fac9', embedding=None, metadata={'page_label': '46', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Jailbreaking examples\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='6436938c-c300-4950-a6df-b9adc73e4d75', embedding=None, metadata={'page_label': '47', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='RLHF•Reinforcement Learning from Human Feedback (RLHF) is now being used to train LLMs that fit human preference •RLHF involves collecting high-quality prompt datasets •Popular examples: •Claude (Anthropic) •ChatGPT (OpenAI)\\nSource: ChatGPT: Optimizing Language Models for Dialogue\\n', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='2788a320-a4f0-447c-ac65-9a3f1a68d0cb', embedding=None, metadata={'page_label': '48', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Future Directions•Augmented LMs •Emergent ability of LMs •Acting / Planning - Reinforcement Learning •Multimodal Prompting •Graph Prompting', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='d2fe400c-ff82-4106-9aaa-2d7530efea04', embedding=None, metadata={'page_label': '49', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='What’s next?•This is the first lecture in a series of lectures •Subscribe to the channel •Follow us on Twitter (@dair_ai and @omarsar0) for announcements on upcoming special lectures covering a variety of topics in language models •The lectures, guides, notebooks, and other prompt engineering content will live on the Prompt Engineering Guide: •https://github.com/dair-ai/Prompt-Engineering-Guide', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n'),\n"," Document(id_='0e70f352-2833-447c-ab58-7204c5652857', embedding=None, metadata={'page_label': '50', 'file_name': 'Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_path': '/content/data/Prompt-Engineering-Lecture-Elvis (1).pdf', 'file_type': 'application/pdf', 'file_size': 3164942, 'creation_date': '2024-05-07', 'last_modified_date': '2024-05-07'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, text='Prompt Engineering Guide\\nhttps://github.com/dair-ai/Prompt-Engineering-Guide', start_char_idx=None, end_char_idx=None, text_template='{metadata_str}\\n\\n{content}', metadata_template='{key}: {value}', metadata_seperator='\\n')]"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["documents"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["f63cc61c85a645f792db46e253897085","e0fa78d4d02e4076b12dfac373bcec0a","76a604cd2426482c9ab94fea037863b8","13f7d2d8d25145a39fb62fc5791f8c32","757bde86748d467abae3a77cd2eb8b36","a49f1ecb0389430482aac101e9e0812c","6e0177c4c218438b8870b54eba0fb8e2","a17421e150da47419864a9ad693d589d","fe59db681b3a48a68ce05ec328334c54","508045751ebe480f9977fd5b51ddac59","d08a847330b743d6a6beb9ddcdc1f71a","b25d61376cdc4906963f4e81c4b4aad6","be306630fda849fb8de1cfa30f78e784","e6716d30030a43e6b6e8d4c1f4e308cc","5cf21c717d38475fb76d0f214d5d9e97","d6b9908c56b046fab568a9372a9bed91","e636ad59999542559d0c8ddd47c1b1d8","eb538798c70a40328a1981103a0f9453","2d432b3fb0da4bc1992accc4a40fda47","7833a23be4c64cf2b303d74d65ad73cc","a99688e8541c4d96a32179b61df45f8a","26579513daea4aeab87acdd50866acab"]},"executionInfo":{"elapsed":1051,"status":"ok","timestamp":1715061951025,"user":{"displayName":"Jomon Jesudas","userId":"04523114763155887947"},"user_tz":-330},"id":"wdsb-C6GPO6g","outputId":"a1ccb508-c76b-431c-aa3d-ddd0454de01e"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f63cc61c85a645f792db46e253897085","version_major":2,"version_minor":0},"text/plain":["Parsing nodes:   0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b25d61376cdc4906963f4e81c4b4aad6","version_major":2,"version_minor":0},"text/plain":["Generating embeddings:   0%|          | 0/50 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["index=VectorStoreIndex.from_documents(documents,show_progress=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":387,"status":"ok","timestamp":1715061952954,"user":{"displayName":"Jomon Jesudas","userId":"04523114763155887947"},"user_tz":-330},"id":"olVVc_GdPXS6","outputId":"dfba7786-c6ab-4d64-d2a0-7162c72e85cd"},"outputs":[{"data":{"text/plain":["<llama_index.core.indices.vector_store.base.VectorStoreIndex at 0x7a3b1a23e590>"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["index"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jDumj7-5Pc0l"},"outputs":[],"source":["query_engine=index.as_query_engine()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9tk4WCxPqyu"},"outputs":[],"source":["from llama_index.core.retrievers import VectorIndexRetriever\n","from llama_index.core.query_engine import RetrieverQueryEngine\n","from llama_index.core.postprocessor import SimilarityPostprocessor\n","\n","retriever=VectorIndexRetriever(index=index,similarity_top_k=4)\n","postprocessor=SimilarityPostprocessor(similarity_cutoff=0.80)\n","\n","query_engine=RetrieverQueryEngine(retriever=retriever,\n","                                  node_postprocessors=[postprocessor])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2OVgy90kQeEm"},"outputs":[],"source":["response=query_engine.query(\"how to do a few shot prompt and show a sample\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1715062090869,"user":{"displayName":"Jomon Jesudas","userId":"04523114763155887947"},"user_tz":-330},"id":"koTY-vyNREPF","outputId":"0f53c099-271b-4e37-e9c3-46c351598195"},"outputs":[{"name":"stdout","output_type":"stream","text":["Final Response: To do a few-shot prompt, you can provide exemplars in\n","prompts to guide the model towards better performance. For example,\n","you can present a set of numbers and ask whether the odd numbers in\n","the group add up to an even number.\n","______________________________________________________________________\n","Source Node 1/2\n","Node ID: 129359c4-6de3-4dae-a8b3-64ca57492e80\n","Similarity: 0.8095783853057031\n","Text: Few-shot Prompts•Few-shot prompting allows us to provide\n","exemplars in prompts to steer the model towards better performanceThe\n","odd numbers in this group add up to an even number: 4, 8, 9, 15, 12,\n","2, 1.A: The answer is False.The odd numbers in this group add up to an\n","even number: 17,  10, 19, 4, 8, 12, 24.A: The answer is True.The odd\n","numbers in ...\n","______________________________________________________________________\n","Source Node 2/2\n","Node ID: ff3329da-dd17-4125-b2b0-04b0f98cd780\n","Similarity: 0.8040738545468594\n","Text: Prompt Engineering Techniques•Many advanced prompting techniques\n","have been designed to improve performance on complex tasks •Few-shot\n","prompts •Chain-of-thought (CoT) prompting •Self-Consistency •Knowledge\n","Generation Prompting •ReAct\n","To do a few-shot prompt, you can provide exemplars in prompts to guide the model towards better performance. For example, you can present a set of numbers and ask whether the odd numbers in the group add up to an even number.\n"]}],"source":["from llama_index.core.response.pprint_utils import pprint_response\n","pprint_response(response,show_source=True)\n","print(response)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HW3DkRkgRImJ"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"13f7d2d8d25145a39fb62fc5791f8c32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_508045751ebe480f9977fd5b51ddac59","placeholder":"​","style":"IPY_MODEL_d08a847330b743d6a6beb9ddcdc1f71a","value":" 50/50 [00:00&lt;00:00, 741.12it/s]"}},"26579513daea4aeab87acdd50866acab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d432b3fb0da4bc1992accc4a40fda47":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"508045751ebe480f9977fd5b51ddac59":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cf21c717d38475fb76d0f214d5d9e97":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a99688e8541c4d96a32179b61df45f8a","placeholder":"​","style":"IPY_MODEL_26579513daea4aeab87acdd50866acab","value":" 50/50 [00:00&lt;00:00, 76.65it/s]"}},"6e0177c4c218438b8870b54eba0fb8e2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"757bde86748d467abae3a77cd2eb8b36":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76a604cd2426482c9ab94fea037863b8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a17421e150da47419864a9ad693d589d","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fe59db681b3a48a68ce05ec328334c54","value":50}},"7833a23be4c64cf2b303d74d65ad73cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a17421e150da47419864a9ad693d589d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a49f1ecb0389430482aac101e9e0812c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a99688e8541c4d96a32179b61df45f8a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b25d61376cdc4906963f4e81c4b4aad6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be306630fda849fb8de1cfa30f78e784","IPY_MODEL_e6716d30030a43e6b6e8d4c1f4e308cc","IPY_MODEL_5cf21c717d38475fb76d0f214d5d9e97"],"layout":"IPY_MODEL_d6b9908c56b046fab568a9372a9bed91"}},"be306630fda849fb8de1cfa30f78e784":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e636ad59999542559d0c8ddd47c1b1d8","placeholder":"​","style":"IPY_MODEL_eb538798c70a40328a1981103a0f9453","value":"Generating embeddings: 100%"}},"d08a847330b743d6a6beb9ddcdc1f71a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6b9908c56b046fab568a9372a9bed91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0fa78d4d02e4076b12dfac373bcec0a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a49f1ecb0389430482aac101e9e0812c","placeholder":"​","style":"IPY_MODEL_6e0177c4c218438b8870b54eba0fb8e2","value":"Parsing nodes: 100%"}},"e636ad59999542559d0c8ddd47c1b1d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e6716d30030a43e6b6e8d4c1f4e308cc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d432b3fb0da4bc1992accc4a40fda47","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7833a23be4c64cf2b303d74d65ad73cc","value":50}},"eb538798c70a40328a1981103a0f9453":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f63cc61c85a645f792db46e253897085":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0fa78d4d02e4076b12dfac373bcec0a","IPY_MODEL_76a604cd2426482c9ab94fea037863b8","IPY_MODEL_13f7d2d8d25145a39fb62fc5791f8c32"],"layout":"IPY_MODEL_757bde86748d467abae3a77cd2eb8b36"}},"fe59db681b3a48a68ce05ec328334c54":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}
